{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set config.gpu_options.allow_growth to True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from time import time\n",
    "from queue import Queue\n",
    "from collections import namedtuple\n",
    "\n",
    "sys.path.append('/home/huy/capstone/godofeye/lib')\n",
    "sys.path.append('/home/huy/capstone/godofeye/lib/yoloface')\n",
    "\n",
    "from blueeyes.face_recognition import FaceDetector, FaceRecognition, FeatureExtractor, ModelTraining\n",
    "from blueeyes.utils import Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Crop from Images (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_DIR = '/home/huy/data/face_recog/train_test_raw/'\n",
    "OUTPUT_DIR = '/home/huy/data/face_recog/train_test'\n",
    "\n",
    "detector = FaceDetector('mtcnn', min_face_size=50)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for img_path in glob.glob(IMAGES_DIR + '/**/*.jpg', recursive=True):\n",
    "    path = Path(img_path)\n",
    "    id = path.parent.name\n",
    "    im = cv2.imread(str(path), 1)\n",
    "    boxes = detector.detect(im)\n",
    "    for left,top,right,bottom in boxes:\n",
    "        crop = im[top:bottom,left:right,:]\n",
    "        output_dir = OUTPUT_DIR + f'/{id}'\n",
    "        output_path = output_dir + f'/{count}.jpg'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        cv2.imwrite(output_path, crop)\n",
    "        print('Write to ', output_path)\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_set_dict = {}\n",
    "test_set_dict = {}\n",
    "\n",
    "TRAINSET_LOCATION = '/home/huy/Downloads/CBGVDataset_v3.2/Aug3/*/WM/*.jpg'\n",
    "TESTSET_LOCATION = '/home/huy/smartbuilding/face_recog_models/dataset/CBGVDataset_v2/*/WM/test/*.jpg'\n",
    "\n",
    "for path in glob.glob(TRAINSET_LOCATION):\n",
    "    path = Path(path)\n",
    "    id = path.parent.parent.parent.name\n",
    "    if id not in train_set_dict.keys():\n",
    "        train_set_dict[id] = []\n",
    "    train_set_dict[id].append(str(path)) \n",
    "for path in glob.glob(TESTSET_LOCATION):\n",
    "    path = Path(path)\n",
    "    id = path.parent.parent.parent.name\n",
    "    if id not in test_set_dict.keys():\n",
    "        test_set_dict[id] = []\n",
    "    test_set_dict[id].append(str(path))\n",
    "\n",
    "# for entry in os.scandir('/home/huy/face_recog/dataset/Data v4.1/train_set_mix'):\n",
    "#     id = entry.name\n",
    "#     train_paths = []\n",
    "#     test_paths = []\n",
    "#     all_paths = glob.glob(os.path.join(entry.path, '*'))\n",
    "#     np.random.shuffle(all_paths)\n",
    "#     for path in all_paths[2:len(all_paths)]:\n",
    "#         train_paths.append(os.path.abspath(path))\n",
    "#     for path in all_paths[0:2]:\n",
    "#         test_paths.append(os.path.abspath(path))\n",
    "# #     for path in all_paths:\n",
    "# #         train_paths.append(os.path.abspath(path))\n",
    "#     train_set_dict[id] = train_paths\n",
    "#     test_set_dict[id] = test_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto split train test\n",
    "from pathlib import Path\n",
    "\n",
    "RATIO = 1.0\n",
    "\n",
    "all_set_dict = {}\n",
    "train_set_dict = {}\n",
    "test_set_dict = {}\n",
    "\n",
    "# TRAINSET_LOCATION = '/home/huy/Downloads/StaffDATA_v1(CBGVDataset_v3.1)/Aug2/**/*.jpg'\n",
    "# TRAINSET_LOCATION = '/home/huy/Downloads/CBGVDataset_v3.2/Aug3/*/WM/*.jpg'\n",
    "TRAINSET_LOCATION = '/home/huy/Downloads/StaffDATA_v1(CBGVDataset_v3.1)/CBGVDataset_v3/*/WM/test/*.jpg'\n",
    "\n",
    "for path in glob.glob(TRAINSET_LOCATION, recursive=True):\n",
    "    path = Path(path)\n",
    "    id = path.parent.parent.name\n",
    "    if id not in all_set_dict.keys():\n",
    "        all_set_dict[id] = []\n",
    "    all_set_dict[id].append(str(path)) \n",
    "\n",
    "for label, paths in all_set_dict.items():\n",
    "    n = int(len(paths)*RATIO)\n",
    "    train_set_dict[label] = paths[0:n]\n",
    "    test_set_dict[label] = paths[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = FeatureExtractor('face_recognition')\n",
    "model_trainer = ModelTraining(feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb786e9f7c944b893488444b2aca985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28bb7fd32234c23946cfc5ce764b7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0903ac3eae88481cbe2eda68c3336753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a06d5f783ca4e5a9a288a7d312951d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be657944e2d84aa9b5836b01a1f91694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec32a2afd9e4a5ab577f323791e6139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4279d09ba34f2c925231abd62bb546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d60e629ca024d45a6bfbd1faeafcd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe2d66c727b4ccb89df628cf9ced065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130cec0f75ce45f5946fec0cdd3034a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11/80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adeffb6008c74004af8d5678949b85b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a62d48877127>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_train_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_model_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/huy/face_recog/encoded_data/data_v3.2_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/capstone/godofeye/lib/blueeyes/face_recognition/recognition.py\u001b[0m in \u001b[0;36mcreate_train_set\u001b[0;34m(self, train_set_dict, detect_face, output_model_location)\u001b[0m\n\u001b[1;32m    388\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdetect_face\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mencoded_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                             \u001b[0mencoded_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/capstone/godofeye/lib/blueeyes/face_recognition/recognition.py\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mknown_face_box\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_face_locations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknown_face_box\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mobilenet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/cv/lib/python3.8/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36mface_encodings\u001b[0;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \"\"\"\n\u001b[1;32m    213\u001b[0m     \u001b[0mraw_landmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_face_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_landmark_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jitters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mraw_landmark_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_landmarks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/cv/lib/python3.8/site-packages/face_recognition/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \"\"\"\n\u001b[1;32m    213\u001b[0m     \u001b[0mraw_landmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_raw_face_landmarks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_face_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_face_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_landmark_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_jitters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mraw_landmark_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_landmarks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_trainer.create_train_set(train_set_dict, output_model_location='/home/huy/face_recog/encoded_data/data_v3.2_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.create_train_set(test_set_dict, output_model_location='/home/huy/face_recog/encoded_data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATION = '/home/huy/Downloads/StaffDATA_v1(CBGVDataset_v3.1)/CBGVDataset_v3/*/WM/*.jpg'\n",
    "data_dict = {}\n",
    "for path in glob.glob(LOCATION):\n",
    "    path = Path(path)\n",
    "    id = path.parent.parent.name\n",
    "    if id not in data_dict.keys():\n",
    "        data_dict[id] = []\n",
    "    data_dict[id].append(str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.create_train_set(data_dict, output_model_location='/home/huy/face_recog/encoded_data/test_wm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load('/home/huy/face_recog/encoded_data/train/features.dat')\n",
    "labels = np.array(open('/home/huy/face_recog/encoded_data/train/labels.dat').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=300, p=2,\n",
      "                     weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "model_trainer.train_knn(features, labels, K=300, weights='uniform', output_model_location='/home/huy/face_recog/models/knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_trainer.train_simple_model(features, labels, output_model_location='/home/huy/face_recog/models/simple_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog = FaceRecognition(\n",
    "    classifier_method='nn'\n",
    ")\n",
    "TP_count = 0\n",
    "UNK_count = 0\n",
    "num_samples = 0\n",
    "for id, img_paths in test_set_dict.items():\n",
    "    for path in img_paths:\n",
    "        img = cv2.imread(path, 1)\n",
    "        predict_id = recog.recog(img,[[0,0,img.shape[1],img.shape[0]]], threshold=0.9)\n",
    "        predict_id = predict_id[0][0].split('\\n')[0]\n",
    "        if predict_id == id:\n",
    "            TP_count += 1\n",
    "        elif predict_id == 'unknown':\n",
    "            UNK_count +=1\n",
    "        num_samples += 1\n",
    "# TP rate don't care UNK\n",
    "print('num_samples\\t', 'TP_count\\t', 'UNK_count\\t')\n",
    "print(num_samples, TP_count, UNK_count)\n",
    "print('TP Rate ',TP_count/(num_samples-UNK_count))\n",
    "# False rate\n",
    "# print(1 - (TP_count+UNK_count)/num_samples\n",
    "print('UNK rate ', UNK_count/num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples\t TP_count\t UNK_count\t\n",
      "28887 25379 3198\n",
      "TP Rate  0.9879325781462883\n",
      "UNK rate  0.1107072385502129\n"
     ]
    }
   ],
   "source": [
    "recog = FaceRecognition(\n",
    "    model_dir='/home/huy/face_recog/models/knn/', \n",
    "   classifier_method='knn'\n",
    ")\n",
    "TP_count = 0\n",
    "UNK_count = 0\n",
    "num_samples = 0\n",
    "for id, img_paths in test_set_dict.items():\n",
    "    for path in img_paths:\n",
    "        img = cv2.imread(path, 1)\n",
    "        predict_id = recog.recog(img,[[0,0,img.shape[1],img.shape[0]]], threshold=0.5)\n",
    "        predict_id = predict_id[0][0].split('\\n')[0]\n",
    "        if predict_id == id:\n",
    "            TP_count += 1\n",
    "        elif predict_id == 'unknown':\n",
    "            UNK_count +=1\n",
    "        num_samples += 1\n",
    "# TP rate don't care UNK\n",
    "print('num_samples\\t', 'TP_count\\t', 'UNK_count\\t')\n",
    "print(num_samples, TP_count, UNK_count)\n",
    "print('TP Rate ',TP_count/(num_samples-UNK_count))\n",
    "# False rate\n",
    "# print(1 - (TP_count+UNK_count)/num_samples\n",
    "print('UNK rate ', UNK_count/num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_samples\t TP_count\t UNK_count\t\n",
      "28887 10940 5016\n",
      "TP Rate  0.4582966779774622\n",
      "UNK rate  0.1736421227541801\n"
     ]
    }
   ],
   "source": [
    "recog = FaceRecognition(\n",
    "    model_dir='/home/huy/models/simple_distance/',\n",
    "    feature_extractor_type='face_recognition'\n",
    ")\n",
    "TP_count = 0\n",
    "UNK_count = 0\n",
    "num_samples = 0\n",
    "for id, img_paths in test_set_dict.items():\n",
    "    for path in img_paths:\n",
    "        img = cv2.imread(path, 1)\n",
    "        predict_id = recog.recog(img,[[0,0,img.shape[1],img.shape[0]]], threshold=0.5)\n",
    "        predict_id = predict_id[0][0].split('\\n')[0]\n",
    "        if predict_id == id:\n",
    "            TP_count += 1\n",
    "        elif predict_id == 'unknown':\n",
    "            UNK_count +=1\n",
    "        num_samples += 1\n",
    "# TP rate don't care UNK\n",
    "print('num_samples\\t', 'TP_count\\t', 'UNK_count\\t')\n",
    "print(num_samples, TP_count, UNK_count)\n",
    "print('TP Rate ',TP_count/(num_samples-UNK_count))\n",
    "# False rate\n",
    "# print(1 - (TP_count+UNK_count)/num_samples\n",
    "print('UNK rate ', UNK_count/num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = pickle.load(open('/home/feature = [1]*128huy/Downloads/knn_clf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.random.random((128,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = KNN.predict_proba([feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN.algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
